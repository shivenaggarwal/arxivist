{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inception Module\n",
        "\n",
        "The Inception module introduces a significant departure from earlier architectures like **AlexNet** and **ZF-Net**, which relied on a fixed convolution filter size for each layer. Instead, the Inception architecture performs multiple convolutions with different filter sizes in parallel — specifically **1×1**, **3×3**, and **5×5** — along with a **3×3 max pooling** operation.\n",
        "\n",
        "Each of these filter sizes serves a distinct purpose:\n",
        "\n",
        "- **1×1 convolutions**: Help with dimensionality reduction and capture fine-grained, local features.  \n",
        "- **3×3 convolutions**: Focus on mid-level spatial patterns.  \n",
        "- **5×5 convolutions**: Designed to detect more abstract, broader features.  \n",
        "- **3×3 max pooling**: A commonly used technique in deep networks, included to enhance performance through feature downsampling.\n",
        "\n",
        "All these operations are applied **in parallel** to the same input, and their outputs are **concatenated (depth-wise)** to form the final output of the module.\n",
        "\n",
        "> The key idea: By combining filters of multiple sizes, the Inception module can effectively capture features at different spatial scales, improving its ability to detect both fine details and global patterns."
      ],
      "metadata": {
        "id": "DS-H-vKlJuGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb7z5FayEYuJ",
        "outputId": "d756ea79-810f-48a8-b204-ee7294df795e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "DJ7N0A0vEb4J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GoogLeNet model"
      ],
      "metadata": {
        "id": "CW5VvofUJkk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the basic convolution block\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(ConvBlock, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "# Building the inception block\n",
        "class Inception(nn.Module):\n",
        "  def __init__(self, in_channels, num1x1, num3x3_reduce, num3x3, num5x5_reduce, num5x5, pool_proj):\n",
        "    super(Inception, self).__init__()\n",
        "\n",
        "    # 4 output channel for each parallel block of network\n",
        "    # blocks are ran parallely not sequentially\n",
        "    self.block1 = nn.Sequential(\n",
        "      ConvBlock(in_channels, num1x1, kernel_size=1, stride=1, padding=0)\n",
        "    )\n",
        "    self.block2 = nn.Sequential(\n",
        "      ConvBlock(in_channels, num3x3_reduce, kernel_size=1, stride=1, padding=0),\n",
        "      ConvBlock(num3x3_reduce, num3x3, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "    self.block3 = nn.Sequential(\n",
        "      ConvBlock(in_channels, num5x5_reduce, kernel_size=1, stride=1, padding=0),\n",
        "      ConvBlock(num5x5_reduce, num5x5, kernel_size=5, stride=1, padding=2)\n",
        "    )\n",
        "    self.block4 = nn.Sequential(\n",
        "      nn.MaxPool2d(3, stride=1, padding=1, ceil_mode=True),\n",
        "      ConvBlock(in_channels, pool_proj, kernel_size=1, stride=1, padding=0)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    block1 = self.block1(x)\n",
        "    block2 = self.block2(x)\n",
        "    block3 = self.block3(x)\n",
        "    block4 = self.block4(x)\n",
        "\n",
        "    return torch.cat([block1, block2, block3, block4], 1) # N * filters * 28 * 28 here 1 denotes that we are concatinating the filter size\n",
        "\n",
        "class Auxiliary(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super(Auxiliary, self).__init__()\n",
        "\n",
        "    self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
        "    self.conv = nn.Conv2d(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(2048, 1024)\n",
        "    self.dropout = nn.Dropout(0.7)\n",
        "    self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool(x)\n",
        "    out = self.conv(out)\n",
        "    out = self.relu(out)\n",
        "    # ao, a conv output of [batch_size, num_channel, height, width] should be “flattened” to become a `[batch_size, num_channel x height x width]` tensor.\n",
        "    # and the in_`features` of the linear layer should be set to `[num_channel * height * width]`\n",
        "    out = torch.flatten(out,1) # conv will output a 4 dim tensor but fc1 requires 2 dim vector\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(GoogLeNet, self).__init__()\n",
        "\n",
        "    self.conv1 = ConvBlock(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "    self.pool1 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "    self.conv2 = ConvBlock(64, 64, kernel_size=1, stride=1, padding=0)\n",
        "    self.conv3 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool3 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "    self.inception3A = Inception(in_channels=192, num1x1=64, num3x3_reduce=96, num3x3=128, num5x5_reduce=16, num5x5=32, pool_proj=32)\n",
        "    self.inception3B = Inception(in_channels=256, num1x1=128, num3x3_reduce=128, num3x3=192, num5x5_reduce=32, num5x5=96, pool_proj=64)\n",
        "    self.pool4 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "    self.inception4A = Inception(in_channels=480, num1x1=192, num3x3_reduce=96, num3x3=208, num5x5_reduce=16, num5x5=48, pool_proj=64)\n",
        "    self.inception4B = Inception(in_channels=512, num1x1=160, num3x3_reduce=112, num3x3=224, num5x5_reduce=24, num5x5=64, pool_proj=64)\n",
        "    self.inception4C = Inception(in_channels=512, num1x1=128, num3x3_reduce=128, num3x3=256, num5x5_reduce=24, num5x5=64, pool_proj=64)\n",
        "    self.inception4D = Inception(in_channels=512, num1x1=112, num3x3_reduce=144, num3x3=288, num5x5_reduce=32, num5x5=64, pool_proj=64)\n",
        "    self.inception4E = Inception(in_channels=528, num1x1=256, num3x3_reduce=160, num3x3=320, num5x5_reduce=32, num5x5=128, pool_proj=128)\n",
        "    self.pool5 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "    self.inception5A = Inception(in_channels=832, num1x1=256, num3x3_reduce=160, num3x3=320, num5x5_reduce=32, num5x5=128, pool_proj=128)\n",
        "    self.inception5B = Inception(in_channels=832, num1x1=384, num3x3_reduce=192, num3x3=384, num5x5_reduce=48, num5x5=128, pool_proj=128)\n",
        "    self.pool6 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "    self.fc = nn.Linear(1024, num_classes)\n",
        "    self.aux4A = Auxiliary(512, num_classes)\n",
        "    self.aux4D = Auxiliary(528, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.pool1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.pool3(out)\n",
        "    out = self.inception3A(out)\n",
        "    out = self.inception3B(out)\n",
        "    out = self.pool4(out)\n",
        "    out = self.inception4A(out)\n",
        "    aux1 = self.aux4A(out)\n",
        "    out = self.inception4B(out)\n",
        "    out = self.inception4C(out)\n",
        "    out = self.inception4D(out)\n",
        "    aux2 = self.aux4D(out)\n",
        "    out = self.inception4E(out)\n",
        "    out = self.pool5(out)\n",
        "    out = self.inception5A(out)\n",
        "    out = self.inception5B(out)\n",
        "    out = self.pool6(out)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, aux1, aux2\n"
      ],
      "metadata": {
        "id": "iY3Lrfd-EnEn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Code"
      ],
      "metadata": {
        "id": "QX0VwbzcJqEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer):\n",
        "  EPOCHS = 15\n",
        "  train_samples_num = 45000\n",
        "  val_samples_num = 5000\n",
        "  train_epoch_loss_history, val_epoch_loss_history = [], []\n",
        "\n",
        "  for epoch in range(EPOCHS): # loop for each epoch\n",
        "\n",
        "    train_running_loss = 0\n",
        "    correct_train = 0\n",
        "\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for inputs, labels in train_loader: # loop for each batch\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward pass\n",
        "      prediction0, aux_pred1, aux_pred2 = model(inputs)\n",
        "\n",
        "      # backward pass\n",
        "      real_loss = criterion(prediction0, labels)\n",
        "      aux_loss1 = criterion(aux_pred1, labels)\n",
        "      aux_loss2 = criterion(aux_pred2, labels)\n",
        "\n",
        "      loss = real_loss + 0.3 * aux_loss1 + 0.3 * aux_loss2\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # update the correct values\n",
        "      _, predicted = torch.max(prediction0.data, 1) # dim=1 means across the rows\n",
        "      correct_train += (predicted == labels).float().sum().item()\n",
        "\n",
        "      # uptil now we have calculated the avg loss\n",
        "      # so no we will have to calculate the batch loss as well\n",
        "      # for that we multiply avg batch loss with the batch length\n",
        "      train_running_loss += loss.data.item() * inputs.shape[0] # 0 ele of inputs is always the batch size\n",
        "\n",
        "    train_epoch_loss = train_running_loss / train_samples_num\n",
        "    train_epoch_loss_history.append(train_epoch_loss)\n",
        "\n",
        "    train_acc = correct_train / train_samples_num\n",
        "\n",
        "    val_loss = 0\n",
        "    correct_val = 0\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad(): # computign the val accuracy so we switch off the gradient calculcation\n",
        "      for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass.\n",
        "        prediction0, aux_pred_1, aux_pred_2 = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        real_loss = criterion(prediction0, labels)\n",
        "        aux_loss_1 = criterion(aux_pred_1, labels)\n",
        "        aux_loss_2 = criterion(aux_pred_2, labels)\n",
        "\n",
        "        loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(prediction0.data, 1)\n",
        "        correct_val += (predicted == labels).float().sum().item()\n",
        "\n",
        "        # Compute batch loss\n",
        "        val_loss += loss.data.item() * inputs.shape[0]\n",
        "\n",
        "      val_loss /= val_samples_num\n",
        "      val_epoch_loss_history.append(val_loss)\n",
        "      val_acc = correct_val / val_samples_num\n",
        "\n",
        "    info = \"[For Epoch {}/{}]: train-loss = {:0.5f} | train-acc = {:0.3f} | val-loss = {:0.5f} | val-acc = {:0.3f}\"\n",
        "\n",
        "    print(info.format(epoch + 1, EPOCHS, train_epoch_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "    torch.save(model.state_dict(), \"/content/sample_data/checkpoint{}\".format(epoch + 1))\n",
        "\n",
        "  torch.save(model.state_dict(), \"/content/sample_data/googlenet_model\")\n",
        "\n",
        "  return train_epoch_loss_history, val_epoch_loss_history\n"
      ],
      "metadata": {
        "id": "6LlcwReCFWSE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogLeNet()\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 96, 96))"
      ],
      "metadata": {
        "id": "5cV5pTHqFYLq",
        "outputId": "519a3448-4401-47cf-87ba-58213951ac00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]           9,472\n",
            "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
            "              ReLU-3           [-1, 64, 48, 48]               0\n",
            "         ConvBlock-4           [-1, 64, 48, 48]               0\n",
            "         MaxPool2d-5           [-1, 64, 24, 24]               0\n",
            "            Conv2d-6           [-1, 64, 24, 24]           4,160\n",
            "       BatchNorm2d-7           [-1, 64, 24, 24]             128\n",
            "              ReLU-8           [-1, 64, 24, 24]               0\n",
            "         ConvBlock-9           [-1, 64, 24, 24]               0\n",
            "           Conv2d-10          [-1, 192, 24, 24]         110,784\n",
            "      BatchNorm2d-11          [-1, 192, 24, 24]             384\n",
            "             ReLU-12          [-1, 192, 24, 24]               0\n",
            "        ConvBlock-13          [-1, 192, 24, 24]               0\n",
            "        MaxPool2d-14          [-1, 192, 12, 12]               0\n",
            "           Conv2d-15           [-1, 64, 12, 12]          12,352\n",
            "      BatchNorm2d-16           [-1, 64, 12, 12]             128\n",
            "             ReLU-17           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-18           [-1, 64, 12, 12]               0\n",
            "           Conv2d-19           [-1, 96, 12, 12]          18,528\n",
            "      BatchNorm2d-20           [-1, 96, 12, 12]             192\n",
            "             ReLU-21           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-22           [-1, 96, 12, 12]               0\n",
            "           Conv2d-23          [-1, 128, 12, 12]         110,720\n",
            "      BatchNorm2d-24          [-1, 128, 12, 12]             256\n",
            "             ReLU-25          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-26          [-1, 128, 12, 12]               0\n",
            "           Conv2d-27           [-1, 16, 12, 12]           3,088\n",
            "      BatchNorm2d-28           [-1, 16, 12, 12]              32\n",
            "             ReLU-29           [-1, 16, 12, 12]               0\n",
            "        ConvBlock-30           [-1, 16, 12, 12]               0\n",
            "           Conv2d-31           [-1, 32, 12, 12]          12,832\n",
            "      BatchNorm2d-32           [-1, 32, 12, 12]              64\n",
            "             ReLU-33           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-34           [-1, 32, 12, 12]               0\n",
            "        MaxPool2d-35          [-1, 192, 12, 12]               0\n",
            "           Conv2d-36           [-1, 32, 12, 12]           6,176\n",
            "      BatchNorm2d-37           [-1, 32, 12, 12]              64\n",
            "             ReLU-38           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-39           [-1, 32, 12, 12]               0\n",
            "        Inception-40          [-1, 256, 12, 12]               0\n",
            "           Conv2d-41          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-42          [-1, 128, 12, 12]             256\n",
            "             ReLU-43          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-44          [-1, 128, 12, 12]               0\n",
            "           Conv2d-45          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-46          [-1, 128, 12, 12]             256\n",
            "             ReLU-47          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-48          [-1, 128, 12, 12]               0\n",
            "           Conv2d-49          [-1, 192, 12, 12]         221,376\n",
            "      BatchNorm2d-50          [-1, 192, 12, 12]             384\n",
            "             ReLU-51          [-1, 192, 12, 12]               0\n",
            "        ConvBlock-52          [-1, 192, 12, 12]               0\n",
            "           Conv2d-53           [-1, 32, 12, 12]           8,224\n",
            "      BatchNorm2d-54           [-1, 32, 12, 12]              64\n",
            "             ReLU-55           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-56           [-1, 32, 12, 12]               0\n",
            "           Conv2d-57           [-1, 96, 12, 12]          76,896\n",
            "      BatchNorm2d-58           [-1, 96, 12, 12]             192\n",
            "             ReLU-59           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-60           [-1, 96, 12, 12]               0\n",
            "        MaxPool2d-61          [-1, 256, 12, 12]               0\n",
            "           Conv2d-62           [-1, 64, 12, 12]          16,448\n",
            "      BatchNorm2d-63           [-1, 64, 12, 12]             128\n",
            "             ReLU-64           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-65           [-1, 64, 12, 12]               0\n",
            "        Inception-66          [-1, 480, 12, 12]               0\n",
            "        MaxPool2d-67            [-1, 480, 6, 6]               0\n",
            "           Conv2d-68            [-1, 192, 6, 6]          92,352\n",
            "      BatchNorm2d-69            [-1, 192, 6, 6]             384\n",
            "             ReLU-70            [-1, 192, 6, 6]               0\n",
            "        ConvBlock-71            [-1, 192, 6, 6]               0\n",
            "           Conv2d-72             [-1, 96, 6, 6]          46,176\n",
            "      BatchNorm2d-73             [-1, 96, 6, 6]             192\n",
            "             ReLU-74             [-1, 96, 6, 6]               0\n",
            "        ConvBlock-75             [-1, 96, 6, 6]               0\n",
            "           Conv2d-76            [-1, 208, 6, 6]         179,920\n",
            "      BatchNorm2d-77            [-1, 208, 6, 6]             416\n",
            "             ReLU-78            [-1, 208, 6, 6]               0\n",
            "        ConvBlock-79            [-1, 208, 6, 6]               0\n",
            "           Conv2d-80             [-1, 16, 6, 6]           7,696\n",
            "      BatchNorm2d-81             [-1, 16, 6, 6]              32\n",
            "             ReLU-82             [-1, 16, 6, 6]               0\n",
            "        ConvBlock-83             [-1, 16, 6, 6]               0\n",
            "           Conv2d-84             [-1, 48, 6, 6]          19,248\n",
            "      BatchNorm2d-85             [-1, 48, 6, 6]              96\n",
            "             ReLU-86             [-1, 48, 6, 6]               0\n",
            "        ConvBlock-87             [-1, 48, 6, 6]               0\n",
            "        MaxPool2d-88            [-1, 480, 6, 6]               0\n",
            "           Conv2d-89             [-1, 64, 6, 6]          30,784\n",
            "      BatchNorm2d-90             [-1, 64, 6, 6]             128\n",
            "             ReLU-91             [-1, 64, 6, 6]               0\n",
            "        ConvBlock-92             [-1, 64, 6, 6]               0\n",
            "        Inception-93            [-1, 512, 6, 6]               0\n",
            "AdaptiveAvgPool2d-94            [-1, 512, 4, 4]               0\n",
            "           Conv2d-95            [-1, 128, 4, 4]          65,664\n",
            "             ReLU-96            [-1, 128, 4, 4]               0\n",
            "           Linear-97                 [-1, 1024]       2,098,176\n",
            "             ReLU-98                 [-1, 1024]               0\n",
            "          Dropout-99                 [-1, 1024]               0\n",
            "          Linear-100                   [-1, 10]          10,250\n",
            "       Auxiliary-101                   [-1, 10]               0\n",
            "          Conv2d-102            [-1, 160, 6, 6]          82,080\n",
            "     BatchNorm2d-103            [-1, 160, 6, 6]             320\n",
            "            ReLU-104            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-105            [-1, 160, 6, 6]               0\n",
            "          Conv2d-106            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-107            [-1, 112, 6, 6]             224\n",
            "            ReLU-108            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-109            [-1, 112, 6, 6]               0\n",
            "          Conv2d-110            [-1, 224, 6, 6]         226,016\n",
            "     BatchNorm2d-111            [-1, 224, 6, 6]             448\n",
            "            ReLU-112            [-1, 224, 6, 6]               0\n",
            "       ConvBlock-113            [-1, 224, 6, 6]               0\n",
            "          Conv2d-114             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-115             [-1, 24, 6, 6]              48\n",
            "            ReLU-116             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-117             [-1, 24, 6, 6]               0\n",
            "          Conv2d-118             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-119             [-1, 64, 6, 6]             128\n",
            "            ReLU-120             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-121             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-122            [-1, 512, 6, 6]               0\n",
            "          Conv2d-123             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-124             [-1, 64, 6, 6]             128\n",
            "            ReLU-125             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-126             [-1, 64, 6, 6]               0\n",
            "       Inception-127            [-1, 512, 6, 6]               0\n",
            "          Conv2d-128            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-129            [-1, 128, 6, 6]             256\n",
            "            ReLU-130            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-131            [-1, 128, 6, 6]               0\n",
            "          Conv2d-132            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-133            [-1, 128, 6, 6]             256\n",
            "            ReLU-134            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-135            [-1, 128, 6, 6]               0\n",
            "          Conv2d-136            [-1, 256, 6, 6]         295,168\n",
            "     BatchNorm2d-137            [-1, 256, 6, 6]             512\n",
            "            ReLU-138            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-139            [-1, 256, 6, 6]               0\n",
            "          Conv2d-140             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-141             [-1, 24, 6, 6]              48\n",
            "            ReLU-142             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-143             [-1, 24, 6, 6]               0\n",
            "          Conv2d-144             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-145             [-1, 64, 6, 6]             128\n",
            "            ReLU-146             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-147             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-148            [-1, 512, 6, 6]               0\n",
            "          Conv2d-149             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-150             [-1, 64, 6, 6]             128\n",
            "            ReLU-151             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-152             [-1, 64, 6, 6]               0\n",
            "       Inception-153            [-1, 512, 6, 6]               0\n",
            "          Conv2d-154            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-155            [-1, 112, 6, 6]             224\n",
            "            ReLU-156            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-157            [-1, 112, 6, 6]               0\n",
            "          Conv2d-158            [-1, 144, 6, 6]          73,872\n",
            "     BatchNorm2d-159            [-1, 144, 6, 6]             288\n",
            "            ReLU-160            [-1, 144, 6, 6]               0\n",
            "       ConvBlock-161            [-1, 144, 6, 6]               0\n",
            "          Conv2d-162            [-1, 288, 6, 6]         373,536\n",
            "     BatchNorm2d-163            [-1, 288, 6, 6]             576\n",
            "            ReLU-164            [-1, 288, 6, 6]               0\n",
            "       ConvBlock-165            [-1, 288, 6, 6]               0\n",
            "          Conv2d-166             [-1, 32, 6, 6]          16,416\n",
            "     BatchNorm2d-167             [-1, 32, 6, 6]              64\n",
            "            ReLU-168             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-169             [-1, 32, 6, 6]               0\n",
            "          Conv2d-170             [-1, 64, 6, 6]          51,264\n",
            "     BatchNorm2d-171             [-1, 64, 6, 6]             128\n",
            "            ReLU-172             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-173             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-174            [-1, 512, 6, 6]               0\n",
            "          Conv2d-175             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-176             [-1, 64, 6, 6]             128\n",
            "            ReLU-177             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-178             [-1, 64, 6, 6]               0\n",
            "       Inception-179            [-1, 528, 6, 6]               0\n",
            "AdaptiveAvgPool2d-180            [-1, 528, 4, 4]               0\n",
            "          Conv2d-181            [-1, 128, 4, 4]          67,712\n",
            "            ReLU-182            [-1, 128, 4, 4]               0\n",
            "          Linear-183                 [-1, 1024]       2,098,176\n",
            "            ReLU-184                 [-1, 1024]               0\n",
            "         Dropout-185                 [-1, 1024]               0\n",
            "          Linear-186                   [-1, 10]          10,250\n",
            "       Auxiliary-187                   [-1, 10]               0\n",
            "          Conv2d-188            [-1, 256, 6, 6]         135,424\n",
            "     BatchNorm2d-189            [-1, 256, 6, 6]             512\n",
            "            ReLU-190            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-191            [-1, 256, 6, 6]               0\n",
            "          Conv2d-192            [-1, 160, 6, 6]          84,640\n",
            "     BatchNorm2d-193            [-1, 160, 6, 6]             320\n",
            "            ReLU-194            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-195            [-1, 160, 6, 6]               0\n",
            "          Conv2d-196            [-1, 320, 6, 6]         461,120\n",
            "     BatchNorm2d-197            [-1, 320, 6, 6]             640\n",
            "            ReLU-198            [-1, 320, 6, 6]               0\n",
            "       ConvBlock-199            [-1, 320, 6, 6]               0\n",
            "          Conv2d-200             [-1, 32, 6, 6]          16,928\n",
            "     BatchNorm2d-201             [-1, 32, 6, 6]              64\n",
            "            ReLU-202             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-203             [-1, 32, 6, 6]               0\n",
            "          Conv2d-204            [-1, 128, 6, 6]         102,528\n",
            "     BatchNorm2d-205            [-1, 128, 6, 6]             256\n",
            "            ReLU-206            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-207            [-1, 128, 6, 6]               0\n",
            "       MaxPool2d-208            [-1, 528, 6, 6]               0\n",
            "          Conv2d-209            [-1, 128, 6, 6]          67,712\n",
            "     BatchNorm2d-210            [-1, 128, 6, 6]             256\n",
            "            ReLU-211            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-212            [-1, 128, 6, 6]               0\n",
            "       Inception-213            [-1, 832, 6, 6]               0\n",
            "       MaxPool2d-214            [-1, 832, 3, 3]               0\n",
            "          Conv2d-215            [-1, 256, 3, 3]         213,248\n",
            "     BatchNorm2d-216            [-1, 256, 3, 3]             512\n",
            "            ReLU-217            [-1, 256, 3, 3]               0\n",
            "       ConvBlock-218            [-1, 256, 3, 3]               0\n",
            "          Conv2d-219            [-1, 160, 3, 3]         133,280\n",
            "     BatchNorm2d-220            [-1, 160, 3, 3]             320\n",
            "            ReLU-221            [-1, 160, 3, 3]               0\n",
            "       ConvBlock-222            [-1, 160, 3, 3]               0\n",
            "          Conv2d-223            [-1, 320, 3, 3]         461,120\n",
            "     BatchNorm2d-224            [-1, 320, 3, 3]             640\n",
            "            ReLU-225            [-1, 320, 3, 3]               0\n",
            "       ConvBlock-226            [-1, 320, 3, 3]               0\n",
            "          Conv2d-227             [-1, 32, 3, 3]          26,656\n",
            "     BatchNorm2d-228             [-1, 32, 3, 3]              64\n",
            "            ReLU-229             [-1, 32, 3, 3]               0\n",
            "       ConvBlock-230             [-1, 32, 3, 3]               0\n",
            "          Conv2d-231            [-1, 128, 3, 3]         102,528\n",
            "     BatchNorm2d-232            [-1, 128, 3, 3]             256\n",
            "            ReLU-233            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-234            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-235            [-1, 832, 3, 3]               0\n",
            "          Conv2d-236            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-237            [-1, 128, 3, 3]             256\n",
            "            ReLU-238            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-239            [-1, 128, 3, 3]               0\n",
            "       Inception-240            [-1, 832, 3, 3]               0\n",
            "          Conv2d-241            [-1, 384, 3, 3]         319,872\n",
            "     BatchNorm2d-242            [-1, 384, 3, 3]             768\n",
            "            ReLU-243            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-244            [-1, 384, 3, 3]               0\n",
            "          Conv2d-245            [-1, 192, 3, 3]         159,936\n",
            "     BatchNorm2d-246            [-1, 192, 3, 3]             384\n",
            "            ReLU-247            [-1, 192, 3, 3]               0\n",
            "       ConvBlock-248            [-1, 192, 3, 3]               0\n",
            "          Conv2d-249            [-1, 384, 3, 3]         663,936\n",
            "     BatchNorm2d-250            [-1, 384, 3, 3]             768\n",
            "            ReLU-251            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-252            [-1, 384, 3, 3]               0\n",
            "          Conv2d-253             [-1, 48, 3, 3]          39,984\n",
            "     BatchNorm2d-254             [-1, 48, 3, 3]              96\n",
            "            ReLU-255             [-1, 48, 3, 3]               0\n",
            "       ConvBlock-256             [-1, 48, 3, 3]               0\n",
            "          Conv2d-257            [-1, 128, 3, 3]         153,728\n",
            "     BatchNorm2d-258            [-1, 128, 3, 3]             256\n",
            "            ReLU-259            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-260            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-261            [-1, 832, 3, 3]               0\n",
            "          Conv2d-262            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-263            [-1, 128, 3, 3]             256\n",
            "            ReLU-264            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-265            [-1, 128, 3, 3]               0\n",
            "       Inception-266           [-1, 1024, 3, 3]               0\n",
            "AdaptiveAvgPool2d-267           [-1, 1024, 1, 1]               0\n",
            "         Dropout-268                 [-1, 1024]               0\n",
            "          Linear-269                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 10,348,590\n",
            "Trainable params: 10,348,590\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 22.05\n",
            "Params size (MB): 39.48\n",
            "Estimated Total Size (MB): 61.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading CIFAR-10\n"
      ],
      "metadata": {
        "id": "mJJ63ABAFxSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar_dataloader():\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "\n",
        "  train_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=True, download=True, transform=transform)\n",
        "  test_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "  # Split dataset into training set and validation set.\n",
        "  train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
        "\n",
        "  print(\"Image shape of a random sample image : {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
        "\n",
        "  print(\"Training Set:   {} images\".format(len(train_dataset)))\n",
        "  print(\"Validation Set:   {} images\".format(len(val_dataset)))\n",
        "  print(\"Test Set:       {} images\".format(len(test_dataset)))\n",
        "\n",
        "  BATCH_SIZE = 128\n",
        "\n",
        "  # Generate dataloader\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "5Qf-1i62Fgsl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = cifar_dataloader()"
      ],
      "metadata": {
        "id": "4Ja1WbvJGvfk",
        "outputId": "8ac31023-1ca3-4602-e0aa-571207bb4a42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 27.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set:   45000 images\n",
            "Validation Set:   5000 images\n",
            "Test Set:       10000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "3nQ2o52_G2cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ur2bSB83Gx6f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_loss_history, val_epoch_loss_history = train_model(model, train_loader, val_loader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "duA63HssG5Xf",
        "outputId": "30a2f8c3-04e0-49a2-809e-bddd2c9304a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[For Epoch 1/15]: train-loss = 2.37080 | train-acc = 0.459 | val-loss = 2.03434 | val-acc = 0.553\n",
            "[For Epoch 2/15]: train-loss = 1.69682 | train-acc = 0.629 | val-loss = 1.66142 | val-acc = 0.640\n",
            "[For Epoch 3/15]: train-loss = 1.39172 | train-acc = 0.701 | val-loss = 1.39228 | val-acc = 0.702\n",
            "[For Epoch 4/15]: train-loss = 1.16384 | train-acc = 0.755 | val-loss = 1.31606 | val-acc = 0.724\n",
            "[For Epoch 5/15]: train-loss = 1.00401 | train-acc = 0.789 | val-loss = 1.30383 | val-acc = 0.732\n",
            "[For Epoch 6/15]: train-loss = 0.86572 | train-acc = 0.817 | val-loss = 1.30582 | val-acc = 0.733\n",
            "[For Epoch 7/15]: train-loss = 0.75473 | train-acc = 0.845 | val-loss = 1.27222 | val-acc = 0.742\n",
            "[For Epoch 8/15]: train-loss = 0.65294 | train-acc = 0.864 | val-loss = 1.19393 | val-acc = 0.754\n",
            "[For Epoch 9/15]: train-loss = 0.55171 | train-acc = 0.884 | val-loss = 1.26246 | val-acc = 0.765\n",
            "[For Epoch 10/15]: train-loss = 0.48323 | train-acc = 0.900 | val-loss = 1.27665 | val-acc = 0.762\n",
            "[For Epoch 11/15]: train-loss = 0.42591 | train-acc = 0.912 | val-loss = 1.46086 | val-acc = 0.748\n",
            "[For Epoch 12/15]: train-loss = 0.35146 | train-acc = 0.927 | val-loss = 1.27943 | val-acc = 0.767\n",
            "[For Epoch 13/15]: train-loss = 0.30744 | train-acc = 0.936 | val-loss = 1.41238 | val-acc = 0.764\n",
            "[For Epoch 14/15]: train-loss = 0.27831 | train-acc = 0.943 | val-loss = 1.56182 | val-acc = 0.757\n",
            "[For Epoch 15/15]: train-loss = 0.23903 | train-acc = 0.950 | val-loss = 1.71101 | val-acc = 0.754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "jK4SfcbUHmvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogLeNet()\n",
        "model.load_state_dict(torch.load('/content/sample_data/googlenet_model'))"
      ],
      "metadata": {
        "id": "0h2aFXYcG7NF",
        "outputId": "5499350d-5eab-442a-f6da-48247753ed57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_samples = 10000\n",
        "correct = 0\n",
        "\n",
        "model.eval().cuda()\n",
        "\n",
        "with  torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Make predictions\n",
        "        prediction, _, _ = model(inputs)\n",
        "\n",
        "        # Retrieve predictions indexes\n",
        "        _, predicted_class = torch.max(prediction.data, 1)\n",
        "\n",
        "        # Compute number of correct predictions\n",
        "        correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = correct / num_test_samples\n",
        "\n",
        "print('Test accuracy: {}'.format(test_accuracy))"
      ],
      "metadata": {
        "id": "XkwglTX3HqlJ",
        "outputId": "aedb886b-fdcf-4fcb-c840-1edf77c76d93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAByhKjzHqeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}